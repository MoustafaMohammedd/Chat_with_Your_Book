# ğŸ“š Chat with Your Book

**Chat with Your Book** is an AI-powered **Retrieval-Augmented Generation (RAG)** system built using **FastAPI** and **LangChain**.  
It allows you to upload any book or report (PDF/DOCX), create vector embeddings using a transformer model, and then **ask natural language questions** â€” with accurate, context-based answers retrieved directly from your book.

---

## ğŸš€ Key Features

- ğŸ“˜ **Upload any book/document** (PDF or DOCX)
- ğŸ§© **Automatic text cleaning and chunking**
- ğŸ§  **Embedding generation** using Sentence Transformers
- ğŸ” **Semantic retrieval** with FAISS vector database
- ğŸ’¬ **RAG pipeline** combining retriever + LLM (via OpenRouter or OpenAI)
- âš¡ **FastAPI backend** â€” lightweight, modular, and production-ready
- ğŸ§ª Includes **test scripts** for retriever and generator

---

## ğŸ—ï¸ Project Structure

```

Chat_with_Your_Book/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # Original input files (PDF, DOCX)
â”‚   â”œâ”€â”€ processed/                   # Cleaned and chunked text
â”‚   â””â”€â”€ embeddings/                  # Stored FAISS vector database
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb            # Optional notebook for testing embeddings
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ **init**.py
â”‚   â”œâ”€â”€ config.py                    # Global constants and file paths
â”‚   â”œâ”€â”€ embedder.py                  # Embedding model initialization
â”‚   â”œâ”€â”€ generator.py                 # RAG pipeline logic
â”‚   â”œâ”€â”€ ingest.py                    # Ingests, cleans, chunks, and embeds documents
â”‚   â”œâ”€â”€ retriever.py                 # Loads and queries FAISS vector store
â”‚   â”œâ”€â”€ utils.py                     # Cleaning, file loading, and QA prompt template
â”‚   â””â”€â”€ main.py                      # FastAPI server with all endpoints
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_generator.py            # Unit test for RAG generator
â”‚   â””â”€â”€ test_retriever.py            # Unit test for retriever
â”‚
â”œâ”€â”€ .env.example                     # Example environment variables
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # Project documentation

````

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/MoustafaMohammedd/Chat_with_Your_Book.git
cd Chat_with_Your_Book
````

### 2ï¸âƒ£ Create and activate a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set up your environment variables

Copy `.env.example` â†’ `.env` and add your keys:

```bash
API_KEY="your_api_key_here"
BASE_URL="https://openrouter.ai/api/v1"
MODEL_NAME="openai/gpt-oss-20b:free"
EMBEDD_MODEL_NAME="sentence-transformers/all-MiniLM-L6-v2"
```

> You can use [OpenRouter](https://openrouter.ai) or OpenAI API.

---
Start the FastAPI app:

```bash
uvicorn src.main:app --reload
```

Access the **interactive API docs** at:
ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ğŸ§  Workflow Overview

1. **Ingest**: Upload and process your document.
   â†’ Cleans text, splits into chunks, and creates embeddings.
2. **Retrieve**: Find the most relevant document chunks for a given query.
3. **Generate**: The retriever context + user question are sent to an LLM to produce an accurate answer.

---


## âš¡ FastAPI Endpoints

### **1ï¸âƒ£ Root Endpoint**

`GET /`

```json
{
  "message": "Welcome to the Chat_with_Your_Book Query API!"
}
```

---

### **2ï¸âƒ£ Ingest Document**

`POST /ingest`

```json
{
  "input_file_path": "data/raw/economic-and-investment-monitor-Q4-2022-english.pdf",
  "output_vector_store_path": "data/embeddings/vector_store"
}
```

âœ… **Creates and saves FAISS embeddings** for your document.

---

### **3ï¸âƒ£ Retrieve Chunks**

`POST /retriever_output`

```json
{
  "query": "What is the main point of this document?",
  "vector_store_path": "data/embeddings/vector_store",
  "search_type": "score",
  "n_k": 3,
  "nf_k": 10
}
```

âœ… Returns relevant document chunks for your query.

---

### **4ï¸âƒ£ Ask Your Book**

`POST /ask_your_book`

```json
{
  "question": "What are the key economic insights in Q4 2022?",
  "vector_store_path": "data/embeddings/vector_store"
}
```

âœ… Returns an **LLM-generated answer** using retrieved context.

---

## ğŸ§ª Running the API

Start the FastAPI app:

```bash
uvicorn src.main:app --reload
```

Access the **interactive API docs** at:
ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ğŸ§  Example Query Flow

1. Run `/ingest` once to build the vector store.
2. Use `/ask_your_book` for any number of questions.
3. The API fetches top relevant chunks + LLM answer.

---

## ğŸ§ª Tests

### Run tests manually:

```bash
python tests/test_retriever.py
python tests/test_generator.py
```

### Example output:

```
Answer: The main point of this document is...
```

---


## ğŸ“š Example File Used

Located in `data/raw/`:

```
economic-and-investment-monitor-Q4-2022-english.pdf
```

---

## ğŸ”® Future Enhancements

* ğŸ§© Add conversation memory (multi-turn RAG)
* ğŸ” Add authentication and user management
* ğŸŒ Add **UI** for easy upload and visualization

---
